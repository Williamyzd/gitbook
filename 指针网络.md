指针网络

![在这里插入图片描述](https://img-blog.csdnimg.cn/2019060318165429.png)

传统的序列到序列模型无法解决OOV的问题，即出现词表外单词，decoder的单词生成将会出现困难；指针模型通过指针函数对输入的源数据产生一个生成单词还是复制单词的概率。pgen用来对词汇分布和注意力分布进行加权平均，我们得到扩展词汇表上的以下概率分布：

![在这里插入图片描述](https://img-blog.csdnimg.cn/2019060318192230.png)

与基于 attention 的端到端系统相比，指针生成网络具有以下优点：

* 指针生成网络让从源文本生成单词变得更加容易。这个网络仅需要将足够多的 attention 集中在相关的单词上，并且让pgen 足够的大。
* 指针生成网络甚至可以复制原文本中的非正式单词。这是此方法带给我我们的主要福利，让我们能够处理那些没出现过的单词，同时也允许我们使用更小规模的词汇集（需要较少的计算资源和存储空间）。
* 指针生成网络能够被更快地训练，尤其是训练的前几个阶段。

Coverage mechanism（汇聚机制）

重复是sequenceto-sequence模型的常见问题，本文中采用coverage 机制来解决，在我们的coverage model中，主要维持coverage vector，是之前所有解码器时间步的注意力分配总和也就是某个特定的源单词的收敛就是到此刻它所受到 attention 的和：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190603182933210.png)

ct是对源文档单词的分布，它表示到目前为止这些单词从关注机制接收到的覆盖程度。注意，c0是一个零向量，因为在第一个时间步骤中，没有覆盖任何源文档。
覆盖向量用作注意机制，将方程式（1）改为：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190603183100218.png)

其中wc是和v长度一样的学习参数。这确保注意力机制当前的决定（选择下一个注意点）通过提醒前一个决定而得到通知。即在下一次选择之前，应用了以前注意力的信息，这应该使注意力机制更容易避免重复关注同一地点，从而避免生成重复文本。
定义一个coverage loss 来对同一地点的重复惩罚

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190603202626547.png)

coverage loss是有边界的，colosst≤∑i ati=1，我们假设应该有一个大致一对一的转换率；因此，如果最终覆盖向量大于或小于1，则会受到惩罚。
我们的损失函数更灵活：因为总结不需要统一的覆盖，我们只惩罚到目前为止每个注意力分布和覆盖之间的重叠-防止重复的注意力。最后，在主损失函数中加入由超参数λ重新加权的coverage loss，得到一个新的复合损失函数：

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190603203244142.png)